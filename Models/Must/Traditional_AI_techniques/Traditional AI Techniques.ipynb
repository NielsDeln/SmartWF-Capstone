{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional AI Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import importlib\n",
    "\n",
    "# Construct the path\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..'))\n",
    "# print(path)\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path)\n",
    "# Change the working directory\n",
    "os.chdir(path)\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, make_scorer\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HugoP\\Desktop\\SmartWF-Capstone\\Models\\Must\\DEL_must_model_rep_0.csv\n",
      "C:\\Users\\HugoP\\Desktop\\SmartWF-Capstone\\Models\\Must\\DEL_must_model_rep_2.csv\n",
      "C:\\Users\\HugoP\\Desktop\\SmartWF-Capstone\\Models\\Must\\DEL_must_model_rep_3.csv\n",
      "C:\\Users\\HugoP\\Desktop\\SmartWF-Capstone\\Models\\Must\\DEL_must_model_rep_4_first_part.csv\n",
      "C:\\Users\\HugoP\\Desktop\\SmartWF-Capstone\\Models\\Must\\DEL_must_model_rep_4_second_part.csv\n",
      "C:\\Users\\HugoP\\Desktop\\SmartWF-Capstone\\Models\\Must\\DEL_must_model_rep_5.csv\n",
      "C:\\Users\\HugoP\\Desktop\\SmartWF-Capstone\\Models\\Must\\DEL_must_model_rep_6.csv\n",
      "      Unnamed: 0  Windspeed  STDeV         Leq_x          Leq_y       Leq_res\n",
      "0           1520        5.0   1.25  45760.188360   21553.805731  25909.377131\n",
      "1           1521        5.0   1.50  45315.132941   27812.507289  28849.313984\n",
      "2           1523        5.0   2.00  45315.765722   27327.867252  28499.592373\n",
      "3           1522        5.0   1.75  45397.825938   25967.628431  27488.263998\n",
      "4           1518        5.0   0.75  45072.967985   18310.332750  22772.213444\n",
      "...          ...        ...    ...           ...            ...           ...\n",
      "5540         704       25.0   2.00  55988.195342  119163.151028  79052.627899\n",
      "5541         701       25.0   1.25  51305.946629  128818.870723  79454.907882\n",
      "5542         700       25.0   1.00  52245.073792  127134.576655  79461.751806\n",
      "5543         702       25.0   1.50  55152.165472  131844.983240  80790.811613\n",
      "5544         705       25.0   2.25  59590.420626  140811.222719  84037.556940\n",
      "\n",
      "[5545 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Construct one complete dataset\n",
    "path = r\"C:\\Users\\HugoP\\Desktop\\SmartWF-Capstone\\Models\\Must\\\\\" # use your path\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, index_col=None, header=0, sep=\"\\t\")\n",
    "    li.append(df)\n",
    "\n",
    "must_df = pd.concat(li, axis=0, ignore_index=True)\n",
    "print(must_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose wich target_DEL to train the models on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_DEL = 'Leq_x'\n",
    "target_DEL = 'Leq_y'\n",
    "# target_DEL = 'Leq_res'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting,\n",
    "Train-Validation-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in must_df_multiple occur 6 times: True\n",
      "All items in must_df_unique occur 1 time: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HugoP\\AppData\\Local\\Temp\\ipykernel_6792\\1859678950.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  must_df_multiple['stratify_col'] = must_df_multiple['Windspeed'].astype(str) + '_' + must_df_multiple['STDeV'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in unique Windspeed-STDeV combinations and repeated combinations\n",
    "selection_multiple = ((must_df['Windspeed'] >= 15) & (must_df['STDeV'] >= 1.00))\n",
    "must_df_multiple = must_df[selection_multiple]\n",
    "must_df_unique = must_df[~selection_multiple]\n",
    "\n",
    "print(\"All items in must_df_multiple occur 6 times:\",\n",
    "    (must_df_multiple.groupby(['Windspeed', 'STDeV']).size().reset_index(name='count')['count']==6).all())\n",
    "print(\"All items in must_df_unique occur 1 time:\",\n",
    "      (must_df_unique.groupby(['Windspeed', 'STDeV']).size().reset_index(name='count')['count']==1).all())\n",
    "\n",
    "# Split repeated combinations (3-1-2 distribution at each STDeV value) \n",
    "must_df_multiple['stratify_col'] = must_df_multiple['Windspeed'].astype(str) + '_' + must_df_multiple['STDeV'].astype(str)\n",
    "X_multiple = must_df_multiple[['Windspeed', 'STDeV', 'stratify_col']]\n",
    "y_multiple = must_df_multiple[target_DEL]\n",
    "\n",
    "X_train_multiple, X_test_val_multiple, y_train_multiple, y_test_val_multiple = train_test_split(X_multiple, y_multiple, train_size=2/6, stratify=must_df_multiple['stratify_col'])\n",
    "X_val_multiple, X_test_multiple, y_val_multiple, y_test_multiple = train_test_split(X_test_val_multiple, y_test_val_multiple, test_size=2/4, stratify=X_test_val_multiple['stratify_col'])\n",
    "\n",
    "# Drop stratify column\n",
    "X_train_multiple = X_train_multiple.drop(columns=['stratify_col'])\n",
    "X_test_val_multiple = X_test_val_multiple.drop(columns=['stratify_col'])\n",
    "X_val_multiple = X_val_multiple.drop(columns=['stratify_col'])\n",
    "X_test_multiple = X_test_multiple.drop(columns=['stratify_col'])\n",
    "\n",
    "# Split unique combinations\n",
    "X_unique = must_df_unique[['Windspeed', 'STDeV']]\n",
    "y_unique = must_df_unique[target_DEL]\n",
    "\n",
    "X_train_unique, X_test_val_unique, y_train_unique, y_test_val_unique = train_test_split(X_unique, y_unique,train_size=3/6)\n",
    "X_val_unique, X_test_unique, y_val_unique, y_test_unique = train_test_split(X_test_val_unique, y_test_val_unique, test_size=2/3)\n",
    "\n",
    "# Combine to final datasets\n",
    "X_train = pd.concat((X_train_unique, X_train_multiple))\n",
    "X_val = pd.concat((X_val_unique, X_val_multiple))\n",
    "X_test = pd.concat((X_test_unique, X_test_multiple))\n",
    "\n",
    "y_train = pd.concat((y_train_unique, y_train_multiple))\n",
    "y_val = pd.concat((y_val_unique, y_val_multiple))\n",
    "y_test = pd.concat((y_test_unique, y_test_multiple))\n",
    "\n",
    "train = pd.concat((X_train, y_train), axis=1)\n",
    "val = pd.concat((X_val, y_val), axis=1)\n",
    "test = pd.concat((X_test, y_test), axis=1)\n",
    "\n",
    "train_val_test_dict = {\"training\": train, \"validation\": val, \"testing\":test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in must_df_multiple occur 6 times: True\n",
      "All items in must_df_unique occur 1 time: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HugoP\\AppData\\Local\\Temp\\ipykernel_6792\\1038652458.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  must_df_multiple['stratify_col'] = must_df_multiple['Windspeed'].astype(str) + '_' + must_df_multiple['STDeV'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in unique Windspeed-STDeV combinations and repeated combinations\n",
    "selection_multiple = ((must_df['Windspeed'] >= 15) & (must_df['STDeV'] >= 1.00))\n",
    "must_df_multiple = must_df[selection_multiple]\n",
    "must_df_unique = must_df[~selection_multiple]\n",
    "\n",
    "print(\"All items in must_df_multiple occur 6 times:\",\n",
    "    (must_df_multiple.groupby(['Windspeed', 'STDeV']).size().reset_index(name='count')['count']==6).all())\n",
    "print(\"All items in must_df_unique occur 1 time:\",\n",
    "      (must_df_unique.groupby(['Windspeed', 'STDeV']).size().reset_index(name='count')['count']==1).all())\n",
    "\n",
    "# Split repeated combinations (4-2 distribution)\n",
    "must_df_multiple['stratify_col'] = must_df_multiple['Windspeed'].astype(str) + '_' + must_df_multiple['STDeV'].astype(str)\n",
    "X_multiple = must_df_multiple[['Windspeed', 'STDeV', 'stratify_col']]\n",
    "y_multiple = must_df_multiple[target_DEL]\n",
    "\n",
    "X_train_multiple, X_test_multiple, y_train_multiple, y_test_multiple = train_test_split(X_multiple, y_multiple, test_size=2/6, stratify=must_df_multiple['stratify_col'])\n",
    "\n",
    "# Drop stratify column\n",
    "X_train_multiple = X_train_multiple.drop(columns=['stratify_col'])\n",
    "X_test_multiple = X_test_multiple.drop(columns=['stratify_col'])\n",
    "\n",
    "# Split unique combinations\n",
    "X_unique = must_df_unique[['Windspeed', 'STDeV']]\n",
    "y_unique = must_df_unique[target_DEL]\n",
    "\n",
    "X_train_unique, X_test_unique, y_train_unique, y_test_unique = train_test_split(X_unique, y_unique,test_size=2/6)\n",
    "\n",
    "# Combine to final datasets\n",
    "X_train = pd.concat((X_train_unique, X_train_multiple))\n",
    "X_test = pd.concat((X_test_unique, X_test_multiple))\n",
    "\n",
    "y_train = pd.concat((y_train_unique, y_train_multiple))\n",
    "y_test = pd.concat((y_test_unique, y_test_multiple))\n",
    "\n",
    "train = pd.concat((X_train, y_train), axis=1)\n",
    "test = pd.concat((X_test, y_test), axis=1)\n",
    "\n",
    "train_test_dict = {\"training\":train, \"testing\": test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "# To test whether Test wheter train_test_split happend correctly:\n",
    "def display_output_in_window(output):\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Output Window\")\n",
    "    text_area = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=200, height=60)\n",
    "    text_area.pack(padx=10, pady=10)\n",
    "    text_area.insert(tk.INSERT, output)\n",
    "    text_area.configure(state='disabled')\n",
    "\n",
    "\n",
    "# test_sets = {\"Training\":train, \"Validation\": val, \"Testing\": test}\n",
    "# test_sets = {\"Training\":X_train, \"Validation\": X_val, \"Testing\": X_test}\n",
    "\n",
    "# test_sets = {\"Training\":y_train, \"Validation\": y_val, \"Testing\": y_test}\n",
    "# for key, set in train_test_dict.items():\n",
    "#     set_df = pd.DataFrame(set, columns=['Windspeed', 'STDeV'])\n",
    "#     set_df = set_df.groupby(['Windspeed', 'STDeV']).size().reset_index(name='count')\n",
    "#     output = \"\"\n",
    "#     output += f\"Length {key} set: {len(set_df)}\\n\"\n",
    "#     output += set_df.to_string()+\"\\n\"\n",
    "#     output += f\"All samples occur 2 times: {(set_df['count']==2).all()}\\n\"\n",
    "#     output += \"Combinations which don't occur 1 or 6 times\\n\"\n",
    "#     output += set_df[(set_df['count'] != 1) & (set_df['count'] != 6)].to_string() + \"\\n\\n\"\n",
    "#     output += \"Combinations which do occur 1 or 6 times\\n\"\n",
    "#     output += set_df[(set_df['count'] == 1) | (set_df['count'] == 6)].to_string() + \"\\n\\n\"\n",
    "#     display_output_in_window(output)\n",
    "# window = tk.Tk()\n",
    "# window.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Several plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Models.Must.Traditional_AI_techniques.Plot_data' from 'c:\\\\Users\\\\HugoP\\\\Desktop\\\\SmartWF-Capstone\\\\Models\\\\Must\\\\Traditional_AI_techniques\\\\Plot_data.py'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Models.Must.Traditional_AI_techniques.Plot_data\n",
    "from Models.Must.Traditional_AI_techniques.Plot_data import *\n",
    "importlib.reload(Models.Must.Traditional_AI_techniques.Plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features\n",
    "train = train_val_test_dict['training']\n",
    "val = train_val_test_dict['validation']\n",
    "test = train_val_test_dict['testing']\n",
    "\n",
    "X_train, y_train = train[['Windspeed', 'STDeV']], train[target_DEL]\n",
    "X_val, y_val = val[['Windspeed','STDeV']], val[target_DEL]\n",
    "X_test, y_test = test[['Windspeed', 'STDeV']], test[target_DEL]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for default KNN model on test set:  7662.856219835084\n",
      "Best parameters:  {'n_neighbors': 17, 'p': 2, 'weights': 'uniform'}\n",
      "Cross-validation scores: [-4283.12117259 -5803.62614968 -8422.47875169 -8678.2664997\n",
      " -9372.31717363]\n",
      "Mean cross-validation score: -7311.961949456541\n",
      "MAE for tuned KNN model on validation set:  8078.159586734355\n",
      "Mean absolute percentage error on validation set: 0.09132378501510992\n"
     ]
    }
   ],
   "source": [
    "# Training without tuning\n",
    "knn_untuned = KNeighborsRegressor()\n",
    "knn_untuned.fit(X_train, y_train)\n",
    "knn_untuned_pred = knn_untuned.predict(X_test)\n",
    "knn_untuned_mae = mean_absolute_error(y_test, knn_untuned_pred)\n",
    "print(\"MAE for default KNN model on test set: \", knn_untuned_mae)\n",
    "\n",
    "# Training, tuning with GridSearchCV\n",
    "knn = KNeighborsRegressor()\n",
    "knn_param_grid = {'n_neighbors': [1,2,3, 5, 10, 15, 16, 17, 18, 20, 50],\n",
    "                  'weights': ['uniform', 'distance'],\n",
    "                  'p': [1, 2]\n",
    "                }\n",
    " \n",
    "#scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "knn_grid_search = GridSearchCV(knn, knn_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "knn_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: \", knn_grid_search.best_params_)\n",
    "knn_best = knn_grid_search.best_estimator_\n",
    "\n",
    "# Validate the best model using cross_val_score\n",
    "CV_scores = cross_val_score(knn_best, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(\"Cross-validation scores:\", CV_scores)\n",
    "print(\"Mean cross-validation score:\", CV_scores.mean())\n",
    "\n",
    "knn_best_pred = knn_best.predict(X_val)\n",
    "knn_best_mae = mean_absolute_error(y_val, knn_best_pred)\n",
    "knn_best_mape = mean_absolute_percentage_error(y_val, knn_best_pred)\n",
    "print(\"MAE for tuned KNN model on validation set: \", knn_best_mae)\n",
    "print(\"Mean absolute percentage error on validation set:\", knn_best_mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling of features\n",
    "X_train_rescaled = scaler.inverse_transform(X_train)\n",
    "X_val_rescaled = scaler.inverse_transform(X_val)\n",
    "X_test_rescaled = scaler.inverse_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several plots of result:\n",
      "xs3: 0\n",
      "xs4: 0\n",
      "xs3: 31\n",
      "xs4: 31\n",
      "xs3: 0\n",
      "xs4: 0\n",
      "xs3: 224\n",
      "xs4: 123\n",
      "xs3: 224\n",
      "xs4: 123\n",
      "xs3: 219\n",
      "xs4: 118\n",
      "xs3: 217\n",
      "xs4: 116\n",
      "xs3: 213\n",
      "xs4: 112\n",
      "xs3: 218\n",
      "xs4: 117\n",
      "xs3: 218\n",
      "xs4: 117\n"
     ]
    }
   ],
   "source": [
    "# all_data = pd.DataFrame(np.column_stack((must_df['Windspeed','STDeV'], must_df[target_DEL])), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "ground_truth = pd.DataFrame(np.column_stack((X_val_rescaled[:,:2], y_val)), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "predictions = pd.DataFrame(np.column_stack((X_val_rescaled[:,:2], knn_best_pred)), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "\n",
    "# ground_truth_average = ground_truth.groupby(['Windspeed', 'STDeV'])['Leq'].mean().reset_index()\n",
    "# predictions_average = predictions.groupby(['Windspeed', 'STDeV'])['Leq'].mean().reset_index()\n",
    "\n",
    "print(\"Several plots of result:\")\n",
    "importlib.reload(Models.Must.Traditional_AI_techniques.Plot_data)\n",
    "from Models.Must.Traditional_AI_techniques.Plot_data import *\n",
    "\n",
    "%matplotlib qt\n",
    "# %matplotlib inline\n",
    "plot_label_pred_3D(ground_truth, predictions, title=f'KNN Regressor\\n{target_DEL}')\n",
    "# plot_err_3D(ground_truth, predictions, title=f'KNN Regressor\\n{target_DEL}')\n",
    "\n",
    "plot_label_pred_2D(ground_truth, predictions, title=f'KNN Regressor\\n{target_DEL}',STDeV=all, W_min=5, W_max=25)\n",
    "# plot_err_2D(ground_truth, predictions, title=f'KNN Regressor\\n{target_DEL}',STDeV=all, error_type='wrt_mean')\n",
    "\n",
    "# plot_mean_error(ground_truth, predictions, title=f'KNN Regressor\\n{target_DEL}', variant='Windspeed', error_type='relative')\n",
    "# plot_mean_error(ground_truth, predictions, title='KNN Regressor\\n{target_DEL}', variant='STDeV', error_type='relative')\n",
    "\n",
    "plot_label_pred_2D_mean(ground_truth, predictions, title=f'KNN Regressor\\n{target_DEL}', W_min=5, W_max=25)\n",
    "# plot_pred_error_2D_mean(ground_truth, predictions, title= f'KNN Regressor \\n{target_DEL}', error_type = 'pred_wrt_mean')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6448\n",
      "3228\n"
     ]
    }
   ],
   "source": [
    "# Since we use crossvalidation, we only need train and test dataset\n",
    "# Input features\n",
    "train = train_test_dict['training']\n",
    "test = train_test_dict['testing']\n",
    "\n",
    "X_train, y_train = train[['Windspeed', 'STDeV']], train[target_DEL]\n",
    "X_test, y_test = test[['Windspeed', 'STDeV']], test[target_DEL]\n",
    "\n",
    "print(X_train.size)\n",
    "print(X_test.size)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for default Random Forest model on test set:  1022.0068214903548\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters: {'criterion': 'absolute_error', 'max_depth': 6, 'min_samples_leaf': 3, 'n_estimators': 50}\n",
      "Cross-validation scores: [ -302.38441758  -796.48756865  -808.88016811 -1231.11395686\n",
      " -1011.8982206 ]\n",
      "Mean cross validation score: 830.1528663581632\n",
      "MAE for tuned Random Forest model on test set:  912.72126678138\n"
     ]
    }
   ],
   "source": [
    "# Training without tuning\n",
    "rf_untuned = RandomForestRegressor()\n",
    "rf_untuned.fit(X_train, y_train)\n",
    "rf_untuned_pred = rf_untuned.predict(X_test)\n",
    "rf_untuned_mae = mean_absolute_error(y_test, rf_untuned_pred)\n",
    "print(\"MAE for default Random Forest model on test set: \", rf_untuned_mae)\n",
    "\n",
    "# Training and tuning with GridSearchCV\n",
    "rf = RandomForestRegressor()\n",
    "parameters_RF = {'criterion' : ['absolute_error'],\n",
    "                        'n_estimators': [20, 30, 40, 50],\n",
    "                        'max_depth' : [5,6, 8, 10],\n",
    "                        'min_samples_leaf': [3,5, 10]\n",
    "}\n",
    "# Best found paramters: {'criterion': 'absolute_error', 'max_depth': 15, 'min_samples_leaf': 5, 'n_estimators': 40\n",
    "rf_grid_search = GridSearchCV(rf, parameters_RF, scoring = 'neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", rf_grid_search.best_params_)\n",
    "rf_best = rf_grid_search.best_estimator_\n",
    "\n",
    "\n",
    "rf_CV_scores = cross_val_score(rf_best, X_train, y_train, cv=5, scoring = 'neg_mean_absolute_error')\n",
    "print(\"Cross-validation scores:\", rf_CV_scores)\n",
    "print(\"Mean cross validation score:\", rf_CV_scores.mean())\n",
    "\n",
    "rf_best_pred = rf_best.predict(X_test)\n",
    "rf_best_mae = mean_absolute_error(y_test, rf_best_pred)\n",
    "print(\"MAE for tuned Random Forest model on test set: \", knn_best_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling of features\n",
    "X_train_rescaled = scaler.inverse_transform(X_train)\n",
    "X_val_rescaled = scaler.inverse_transform(X_val)\n",
    "X_test_rescaled = scaler.inverse_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_best_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot result\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# all_data = pd.DataFrame(np.column_stack((must_df['Windspeed','STDeV'], must_df[target_DEL])), columns=['Windspeed', 'STDeV', 'Leq'])\u001b[39;00m\n\u001b[0;32m      3\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mcolumn_stack((X_test_rescaled[:,:\u001b[38;5;241m2\u001b[39m], y_test)), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTDeV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mcolumn_stack((X_test_rescaled[:,:\u001b[38;5;241m2\u001b[39m], \u001b[43mrf_best_pred\u001b[49m)), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTDeV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m ground_truth_average \u001b[38;5;241m=\u001b[39m ground_truth\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTDeV\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      8\u001b[0m predictions_average \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTDeV\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_best_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot result\n",
    "# all_data = pd.DataFrame(np.column_stack((must_df['Windspeed','STDeV'], must_df[target_DEL])), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "ground_truth = pd.DataFrame(np.column_stack((X_test_rescaled[:,:2], y_test)), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "predictions = pd.DataFrame(np.column_stack((X_test_rescaled[:,:2], rf_best_pred)), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "\n",
    "\n",
    "ground_truth_average = ground_truth.groupby(['Windspeed', 'STDeV'])['Leq'].mean().reset_index()\n",
    "predictions_average = predictions.groupby(['Windspeed', 'STDeV'])['Leq'].mean().reset_index()\n",
    "\n",
    "print(\"Several plots of result:\")\n",
    "importlib.reload(Models.Must.Traditional_AI_techniques.Plot_data)\n",
    "from Models.Must.Traditional_AI_techniques.Plot_data import *\n",
    "\n",
    "# %matplotlib qt\n",
    "%matplotlib inline\n",
    "# plot_label_pred_3D(ground_truth, predictions, title=f'Random Forest\\n{target_DEL}')\n",
    "# plot_err_3D(ground_truth, predictions, title=f'Random Forest\\n{target_DEL}')\n",
    "\n",
    "# plot_label_pred_2D(ground_truth, predictions, title=f'Random Forest\\n{target_DEL}',STDeV=all)\n",
    "# plot_err_2D(ground_truth, predictions, title=f'Random Forest\\n{target_DEL}',STDeV=all, error_type='relative')\n",
    "\n",
    "# plot_mean_error(ground_truth, predictions, title=f'Random Forest\\n{target_DEL}', variant='Windspeed', error_type='relative')\n",
    "# plot_mean_error(ground_truth, predictions, title='Random Forest\\n{target_DEL}', variant='STDeV', error_type='relative')\n",
    "\n",
    "plot_label_pred_2D_mean(ground_truth, predictions, title=f'Random Forest\\n{target_DEL}')\n",
    "# plot_pred_error_2D_mean(ground_truth, predictions, title= f'Random Forest \\n{target_DEL}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6448\n",
      "3228\n"
     ]
    }
   ],
   "source": [
    "# Since we use cross validation, we only need train and test dataset.\n",
    "# Input features\n",
    "train = train_test_dict['training']\n",
    "test = train_test_dict['testing']\n",
    "\n",
    "X_train, y_train = train[['Windspeed', 'STDeV']], train[target_DEL]\n",
    "X_test, y_test = test[['Windspeed', 'STDeV']], test[target_DEL]\n",
    "\n",
    "print(X_train.size)\n",
    "print(X_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('preprocessor', PolynomialFeatures()),\n",
    "    ('estimator', SGDRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for default pipeline parameters on test set:  971.5299443808258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HugoP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'estimator__alpha': 0.01, 'estimator__max_iter': 100, 'preprocessor__degree': 3}\n",
      "Cross-validation scores: [ -480.92630393  -878.12227423  -844.80947658 -1225.36977883\n",
      " -1039.06006815]\n",
      "Mean cross-validation score: 893.6575803460692\n",
      "MAE for tuned Polynomial Regression model on test set:  892.7753022265066\n"
     ]
    }
   ],
   "source": [
    "# Training with default parameters\n",
    "poly_SGD_untuned = pipe\n",
    "poly_SGD_untuned.fit(X_train, y_train)\n",
    "poly_SGD_untuned_pred = poly_SGD_untuned.predict(X_test)\n",
    "poly_SGD_untuned_mae = mean_absolute_error(y_test, poly_SGD_untuned_pred)\n",
    "print(\"MAE for default pipeline parameters on test set: \", poly_SGD_untuned_mae)\n",
    "\n",
    "# Training with GridSearchCV\n",
    "poly_SGD = pipe\n",
    "param_grid = {\n",
    "    'preprocessor__degree': [1, 2, 3, 4, 5, 6, 7, 20],\n",
    "    'estimator__alpha': [1e-5, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'estimator__max_iter': [100, 500, 1000, 2000, 5000, 6000]\n",
    "}\n",
    "\n",
    "poly_SGD_grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "poly_SGD_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", poly_SGD_grid_search.best_params_)\n",
    "poly_SGD_best = poly_SGD_grid_search.best_estimator_\n",
    "\n",
    "# Validate the best model using cross_val_score\n",
    "poly_SGD_CV_scores = cross_val_score(poly_SGD_best, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(\"Cross-validation scores:\", poly_SGD_CV_scores)\n",
    "print(\"Mean cross-validation score:\", poly_SGD_CV_scores.mean())\n",
    "\n",
    "\n",
    "poly_SGD_best_pred = poly_SGD_best.predict(X_test)\n",
    "poly_SGD_best_mae = mean_absolute_error(y_test, poly_SGD_best_pred)\n",
    "print(\"MAE for tuned Polynomial Regression model on test set: \", poly_SGD_best_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'poly_SGD_best_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# all_data = pd.DataFrame(np.column_stack((must_df['Windspeed','STDeV'], must_df[target_DEL])), columns=['Windspeed', 'STDeV', 'Leq'])\u001b[39;00m\n\u001b[0;32m      2\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mcolumn_stack((X_test_rescaled[:,:\u001b[38;5;241m2\u001b[39m], y_test)), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTDeV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mcolumn_stack((X_test_rescaled[:,:\u001b[38;5;241m2\u001b[39m], \u001b[43mpoly_SGD_best_pred\u001b[49m)), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTDeV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m ground_truth_average \u001b[38;5;241m=\u001b[39m ground_truth\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTDeV\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      7\u001b[0m predictions_average \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTDeV\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'poly_SGD_best_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# all_data = pd.DataFrame(np.column_stack((must_df['Windspeed','STDeV'], must_df[target_DEL])), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "ground_truth = pd.DataFrame(np.column_stack((X_test_rescaled[:,:2], y_test)), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "predictions = pd.DataFrame(np.column_stack((X_test_rescaled[:,:2], poly_SGD_best_pred)), columns=['Windspeed', 'STDeV', 'Leq'])\n",
    "\n",
    "\n",
    "ground_truth_average = ground_truth.groupby(['Windspeed', 'STDeV'])['Leq'].mean().reset_index()\n",
    "predictions_average = predictions.groupby(['Windspeed', 'STDeV'])['Leq'].mean().reset_index()\n",
    "\n",
    "print(\"Several plots of result:\")\n",
    "importlib.reload(Models.Must.Traditional_AI_techniques.Plot_data)\n",
    "from Models.Must.Traditional_AI_techniques.Plot_data import *\n",
    "\n",
    "# %matplotlib qt\n",
    "%matplotlib inline\n",
    "# plot_label_pred_3D(ground_truth, predictions, title=f'Polynomial feature extraction SGD\\n{target_DEL}')\n",
    "# plot_err_3D(ground_truth, predictions, title=f'Polynomial feature extraction SGD\\n{target_DEL}')\n",
    "\n",
    "# plot_label_pred_2D(ground_truth, predictions, title=f'Polynomial feature extraction SGD\\n{target_DEL}',STDeV=all)\n",
    "# plot_err_2D(ground_truth, predictions, title=f'Polynomial feature extraction SGD\\n{target_DEL}',STDeV=all, error_type='relative')\n",
    "\n",
    "# plot_mean_error(ground_truth, predictions, title=f'Polynomial feature extraction SGD\\n{target_DEL}', variant='Windspeed', error_type='relative')\n",
    "# plot_mean_error(ground_truth, predictions, title='Polynomial feature extraction SGD\\n{target_DEL}', variant='STDeV', error_type='relative')\n",
    "\n",
    "# plot_label_pred_2D_mean(ground_truth, predictions, title=f'Polynomial feature extraction SGD\\n{target_DEL}')\n",
    "plot_pred_error_2D_mean(ground_truth, predictions, title= f'Polynomial feature extraction SGD \\n{target_DEL}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
