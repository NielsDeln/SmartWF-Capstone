{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.Should._utils import *\n",
    "from Models.Should.Dataset_creation import *\n",
    "from Models.Should.Should_LSTM import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/kaggle/input/should-azimuth-complete-simulations/Must_Should_processed' # ADD DATASET PATH\n",
    "save_directory = '/kaggle/working' # ADD DIRECTORY TO SAVE TRAINED MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_list = os.listdir(dataset_path)\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data, validation_data = split_dataset(file_list, test_size=0.15, validation_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the load axis used\n",
    "load_axis = 'Mxb1'\n",
    "# load_axis = 'Myb1'\n",
    "\n",
    "# Calculate average and standard deviation of training set labels\n",
    "train_labels_mean, train_labels_stdev = calculate_average_and_std(dataset_path, train_data, load_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "datasets = {\n",
    "    'train': Should_Dataset(dataset_path, train_data, load_axis, train_labels_mean, train_labels_stdev),\n",
    "    'validation': Should_Dataset(dataset_path, validation_data, load_axis, train_labels_mean, train_labels_stdev),\n",
    "    'test': Should_Dataset(dataset_path, test_data, load_axis, train_labels_mean, train_labels_stdev)\n",
    "}\n",
    "\n",
    "# Create the dataloader\n",
    "batch_size = 64\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'validation': DataLoader(datasets['validation'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'test': DataLoader(datasets['test'], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "}\n",
    "\n",
    "print(f'The length of the train dataset is: {len(datasets[\"train\"])}')\n",
    "print(f'The length of the validation dataset is: {len(datasets[\"validation\"])}')\n",
    "print(f'The length of the test dataset is: {len(datasets[\"test\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "input_size = 2\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "proj_size = 1\n",
    "model = Should_model(input_size=input_size, \n",
    "                    hidden_size=hidden_size, \n",
    "                    num_layers=num_layers, \n",
    "                    dropout=dropout,\n",
    "                    proj_size=proj_size,\n",
    "                    batch_size=batch_size\n",
    "                    )\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'The specified device is: {device}')\n",
    "print(f'The model architecture is:\\n{model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n_epochs = 50\n",
    "model, train_losses, validation_losses, validation_dels = train(\n",
    "    model,                                      \n",
    "    dataloaders,\n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    n_epochs,\n",
    "    save_directory,\n",
    "    device=device, \n",
    "    early_stopping=5, \n",
    "    print_freq=1,\n",
    ")\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plot_losses(train_losses, validation_losses, validation_dels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR IF YOU WANT TO LOAD A DIFFERENT MODEL\n",
    "## USE CORRESPONDING LOAD AXIS\n",
    "model.load_state_dict(torch.load(f'/kaggle/working/model_{date.today()}_Mxb1.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and plot inference\n",
    "test_loss, test_del_error = evaluate(model, dataloaders['test'], loss_fn, device=device)\n",
    "print(f'The loss over the test set is: {test_loss}\\nThe test DEL error is: {test_del_error}')\n",
    "plot_inference(model, dataloaders['test'], num_inf=3, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
